{
  "request": {
    "project": "new",
    "text": "In this work, we study the internal representations of GANs. 
    To a human observer, a well-trained\nGAN appears to have learned facts about the objects in the image: for example, 
    a door can appear on\na building but not on a tree. We wish to understand how a GAN represents such structure. 
    Do the\nobjects emerge as pure pixel patterns without any explicit representation of objects such as doors and\ntrees, 
    or does the GAN contain internal variables that correspond to the objects that humans perceive?
    \nIf the GAN does contain variables for doors and trees, do those variables cause the generation of\nthose objects, or
    do they merely correlate? How are relationships between objects represented?\nBy carefully examining representation units, 
    we have found that many parts of GAN representations\ncan be interpreted, not only as signals that correlate with object 
    concepts but as variables that have\na causal effect on the synthesis of objects in the output. 
    These interpretable effects can be used to\ncompare, debug, modify, and reason about a GAN model. 
    Our method can be potentially applied to\nother generative models such as VAEs  and RealNVP."
  }
